> Taken from https://github.com/Meelfy/pytorch_pretrained_BERT

This package contains an op-for-op PyTorch reimplementation
of [Google's TensorFlow repository for the BERT model](https://github.com/google-research/bert) that was released
together with the
paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.

This implementation is provided with [Google's pre-trained models](https://github.com/google-research/bert), examples,
notebooks and a command-line interface to load any pre-trained TensorFlow checkpoint for BERT is also provided.